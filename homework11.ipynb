{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {
    "id": "_0m4oIEYlSjr"
   },
   "outputs": [],
   "source": [
    "# Install tensorflow_text, if executed in google colab\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "  !pip install -q -U tensorflow --upgrade # We need a newer tensorflow version to use the causal mask of the multi head attention layer\n",
    "  !pip install -q -U tensorflow-text\n",
    "  !pip install -q -U sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {
    "id": "g6BSKo0jlSjt",
    "outputId": "58756bd2-3f41-48b6-d0e9-67d5c608f0ac",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n",
      "Num GPUs Available:  [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "TensorFlow Version:  2.10.0\n"
     ]
    }
   ],
   "source": [
    "# disable compiler warnings\n",
    "import os\n",
    "\n",
    "# imports \n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_text as tf_text\n",
    "from tensorflow.python.ops.numpy_ops import np_config\n",
    "np_config.enable_numpy_behavior()\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from typing import List\n",
    "import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "import sentencepiece as sp\n",
    "import math\n",
    "import tqdm\n",
    "\n",
    "%load_ext tensorboard\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '0'  # FATAL\n",
    "print(\"Num GPUs Available: \", tf.config.list_physical_devices('GPU'))\n",
    "print(\"TensorFlow Version: \", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Constants / Hyperparameter"
   ],
   "metadata": {
    "collapsed": false,
    "id": "620pm5bqcFaX"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "outputs": [],
   "source": [
    "PREPROCESSED_BIBLE_FILE_NAME = \"bible_preprocessed.txt\"\n",
    "VOCABULARY_SIZE = 3000 # 2000 to 7000\n",
    "SEQUENCE_LENGTH = 64 # 32 to 256\n",
    "BATCH_SIZE = 1024\n",
    "EMBEDDING_OUT = 64 # 64 to 256\n",
    "ATTENTION_HEADS = 4 # 2 to 4\n",
    "TRANSFORMER_DENSE_SIZE = 128 # 32 to 256\n",
    "EPOCHS = 100 # 100 to 600\n",
    "TRAIN_SPLIT = 1.0"
   ],
   "metadata": {
    "id": "_RI1GS_YcFaZ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load data"
   ],
   "metadata": {
    "id": "wX8dmMUooDf3"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {
    "id": "Vo5H3SP8lSjw"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Load file from remote, if notebook is executed inside google colab, otherwise it gets loaded from the local file system\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "  bible_url = \"https://raw.githubusercontent.com/IANNwTF-Group-3/homework11/main/bible.txt\"\n",
    "  response = requests.get(bible_url)\n",
    "  text = response.text\n",
    "else:\n",
    "  file_path = \"bible.txt\"\n",
    "  with open(file_path, \"r\") as f:\n",
    "      text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocessing"
   ],
   "metadata": {
    "id": "WNaI2FJQoPDC"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import re\n",
    "\n",
    "# Lowercase the text\n",
    "text = text.lower()\n",
    "\n",
    "# Remove sentence numeration\n",
    "text = re.sub('[0-9]+:[0-9]+ ', '', text)\n",
    "\n",
    "# Remove special characters\n",
    "for c in \"!'()*,-.0123456789:;?\":\n",
    "  text = text.replace(c, '')\n",
    "\n",
    "# Replace multiple spaces with a single space\n",
    "text = re.sub(' +', ' ', text)\n",
    "\n",
    "sentence_separator = \"sentence-separator-placeholder\"\n",
    "# Remember double line breaks\n",
    "text = re.sub('\\n\\n+', sentence_separator, text)\n",
    "# Remove line breaks\n",
    "text = text.replace('\\n', '')\n",
    "# Substitute sentence line breaks back into text\n",
    "text = text.replace(sentence_separator, '\\n')"
   ],
   "metadata": {
    "id": "5yZAxKWhoQY-"
   },
   "execution_count": 640,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Write text to file for later processing"
   ],
   "metadata": {
    "id": "0xLy8nC1yWBS"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "if os.path.exists(PREPROCESSED_BIBLE_FILE_NAME):\n",
    "  os.remove(PREPROCESSED_BIBLE_FILE_NAME)\n",
    "\n",
    "bible_file = open(PREPROCESSED_BIBLE_FILE_NAME, 'xb')\n",
    "bible_file.write(text.encode(encoding='UTF-8'))\n",
    "bible_file.close()"
   ],
   "metadata": {
    "id": "0yNcNqj5yVR1"
   },
   "execution_count": 641,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tokenize"
   ],
   "metadata": {
    "id": "UGxyhxRYoTAs"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create tokenizer model"
   ],
   "metadata": {
    "id": "iZc3nOsCy9hd"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Pretrained model\n",
    "# sp_model_url = \"https://github.com/tensorflow/text/blob/master/tensorflow_text/python/ops/test_data/fast_sentencepiece.model?raw=true\"\n",
    "# sp_model = requests.get(sp_model_url).content\n",
    "\n",
    "# Self trained model\n",
    "sp_model_name = \"sp_tokenizer\"\n",
    "sp.SentencePieceTrainer.train(input=PREPROCESSED_BIBLE_FILE_NAME, model_prefix=sp_model_name, model_type=\"unigram\", vocab_size=VOCABULARY_SIZE)\n",
    "sp_model = tf.io.gfile.GFile(f\"{sp_model_name}.model\", \"rb\").read()"
   ],
   "metadata": {
    "id": "4zHpREzOy5AM"
   },
   "execution_count": 642,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create tokenizer and bible tokens"
   ],
   "metadata": {
    "id": "fntalvOf6q_w"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "sp_tokenizer = tf_text.SentencepieceTokenizer(sp_model)\n",
    "sp_tokens = sp_tokenizer.tokenize(text)"
   ],
   "metadata": {
    "id": "Zs5wtRRSoSs-"
   },
   "execution_count": 643,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Test tokenizer"
   ],
   "metadata": {
    "id": "UC0nRa4b6k3Y"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "test_tokens = sp_tokenizer.tokenize(\"I Jesus have sent mine angel\".lower())\n",
    "for t in test_tokens:\n",
    "  print(sp_tokenizer.detokenize([t]))"
   ],
   "metadata": {
    "id": "n55Zj_Mi6mhE",
    "outputId": "f262ec96-0b85-4ed1-88d0-bd2140e4b420",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "execution_count": 644,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'i', shape=(), dtype=string)\n",
      "tf.Tensor(b'jesus', shape=(), dtype=string)\n",
      "tf.Tensor(b'have', shape=(), dtype=string)\n",
      "tf.Tensor(b'sent', shape=(), dtype=string)\n",
      "tf.Tensor(b'mine', shape=(), dtype=string)\n",
      "tf.Tensor(b'angel', shape=(), dtype=string)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prepare dataset"
   ],
   "metadata": {
    "id": "e35hUUMh_3qk"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create training data using sliding window"
   ],
   "metadata": {
    "id": "8hY31BhZ7g8u"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "sliding_window = tf_text.sliding_window(data=sp_tokens, width=SEQUENCE_LENGTH + 1, axis=0)\n",
    "\n",
    "# Visualize sliding window\n",
    "print(sliding_window)"
   ],
   "metadata": {
    "id": "FimTDaki7mWf",
    "outputId": "65f5fed3-a348-4792-9604-96fdcaa02024",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "execution_count": 645,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[   3  292  562 ...    4   32  237]\n",
      " [ 292  562    5 ...   32  237    3]\n",
      " [ 562    5  172 ...  237    3  392]\n",
      " ...\n",
      " [ 562    5   51 ...   19   26   57]\n",
      " [   5   51 2700 ...   26   57   28]\n",
      " [  51 2700   32 ...   57   28 1339]], shape=(902189, 65), dtype=int32)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create dataset"
   ],
   "metadata": {
    "id": "egMoAnlq96BD"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "full_dataset = tf.data.Dataset.from_tensor_slices((sliding_window[:,:-1], sliding_window[:,1:]))\n",
    "full_dataset = full_dataset.shuffle(4096)\n",
    "full_dataset = full_dataset.batch(BATCH_SIZE)\n",
    "full_dataset = full_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "train_size = math.floor(len(full_dataset) * TRAIN_SPLIT)\n",
    "train_dataset = full_dataset.take(train_size)"
   ],
   "metadata": {
    "id": "4Q3d204E95tR"
   },
   "execution_count": 646,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Embedding layer"
   ],
   "metadata": {
    "collapsed": false,
    "id": "uNqwRZK4cFao"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "outputs": [],
   "source": [
    "class BibleEmbedding(tf.keras.layers.Layer):\n",
    "  def __init__(self):\n",
    "    super(BibleEmbedding, self).__init__()\n",
    "\n",
    "    self.token_embedding = tf.keras.layers.Embedding(VOCABULARY_SIZE, EMBEDDING_OUT)\n",
    "    self.position_embedding = tf.keras.layers.Embedding(SEQUENCE_LENGTH, EMBEDDING_OUT)\n",
    "\n",
    "  def call(self, input_sequence):\n",
    "    input_range = tf.range(0, input_sequence.shape[1])\n",
    "\n",
    "    return tf.math.add(self.token_embedding(input_sequence), self.position_embedding(input_range))"
   ],
   "metadata": {
    "id": "KPSVQkHkcFao"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Transformer block"
   ],
   "metadata": {
    "collapsed": false,
    "id": "myTb9RNdcFap"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "outputs": [],
   "source": [
    "class TransformerBlock(tf.keras.layers.Layer):\n",
    "  def __init__(self):\n",
    "    super(TransformerBlock, self).__init__()\n",
    "    self.head_attention_layer = tf.keras.layers.MultiHeadAttention(ATTENTION_HEADS, EMBEDDING_OUT)\n",
    "    self.dense = tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(TRANSFORMER_DENSE_SIZE, activation='relu'),\n",
    "      tf.keras.layers.Dense(EMBEDDING_OUT, activation=None)\n",
    "    ])\n",
    "    self.dropout1 = tf.keras.layers.Dropout(0.1)\n",
    "    self.dropout2 = tf.keras.layers.Dropout(0.1)\n",
    "    self.layer_normalization1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layer_normalization2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "  def call(self, data, training):\n",
    "    x = self.head_attention_layer(data, data, use_causal_mask=True)\n",
    "    x = self.dropout1(x, training=training)\n",
    "    x = tf.math.add(x, data)\n",
    "    x = self.layer_normalization1(x)\n",
    "    y = self.dense(x)\n",
    "    y = self.dropout2(y, training=training)\n",
    "    x = tf.math.add(x, y)\n",
    "    return self.layer_normalization2(x)"
   ],
   "metadata": {
    "id": "IWV6ajI5cFap"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Bible model"
   ],
   "metadata": {
    "collapsed": false,
    "id": "U1p78otwcFap"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "outputs": [],
   "source": [
    "class BibleModel(tf.keras.Model):\n",
    "  def __init__(self, tokenizer, optimizer=tf.keras.optimizers.Adam(), loss_function=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)):\n",
    "    super(BibleModel, self).__init__()\n",
    "    self.tokenizer = tokenizer\n",
    "    self.optimizer = optimizer\n",
    "    self.loss_function = loss_function\n",
    "    self.metrics_list = [\n",
    "      tf.keras.metrics.Mean(name=\"loss\"),\n",
    "      tf.keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
    "      tf.keras.metrics.SparseTopKCategoricalAccuracy(3, name=\"top-3-acc\")\n",
    "    ]\n",
    "\n",
    "    self.layer = tf.keras.Sequential([\n",
    "      BibleEmbedding(),\n",
    "      TransformerBlock(),\n",
    "      tf.keras.layers.Dense(VOCABULARY_SIZE, activation=None)\n",
    "    ])\n",
    "\n",
    "  def call(self, data, training=True):\n",
    "    return self.layer(data)\n",
    "\n",
    "  def reset_metrics(self):\n",
    "    for metric in self.metrics:\n",
    "        metric.reset_states()\n",
    "\n",
    "  @tf.function\n",
    "  def train_step(self, data):\n",
    "    x, targets = data\n",
    "\n",
    "    # compute output and loss, train the variables\n",
    "    with tf.GradientTape() as tape:\n",
    "      predictions = self(x, training=True)\n",
    "      loss = self.loss_function(targets, predictions) + tf.reduce_sum(self.losses)\n",
    "\n",
    "    # update trainable variables\n",
    "    gradients = tape.gradient(loss, self.trainable_variables)\n",
    "    self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "\n",
    "    # update metrics\n",
    "    self.metrics_list[0].update_state(loss)\n",
    "\n",
    "    for metric in self.metrics_list[1:]:\n",
    "        metric.update_state(targets, predictions)\n",
    "      \n",
    "    # return a dict with metric information\n",
    "    return {m.name : m.result() for m in self.metrics_list}\n",
    "\n",
    "  def generate_text(self, prompt, output_length, top_k):\n",
    "    generated = prompt\n",
    "    for i in range(output_length):\n",
    "        tokenized_prompt = self.tokenizer.tokenize(generated)\n",
    "        output = self(tf.expand_dims(tokenized_prompt, 0), training=False)\n",
    "        logits = output[:, -1, :]  # select the last token's logits\n",
    "        filtered_logits, top_indices = tf.math.top_k(logits, k=top_k, sorted=True)\n",
    "        chosen_index = tf.random.categorical(filtered_logits, num_samples=1)[-1, 0].numpy()\n",
    "        generated += ' ' + self.tokenizer.detokenize(tf.expand_dims(top_indices[0][chosen_index], 0)).numpy().decode(\"utf-8\")\n",
    "    return generated"
   ],
   "metadata": {
    "id": "fCpw7kfXcFap"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training loop"
   ],
   "metadata": {
    "id": "kOAv_LQM1VKc"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def training_loop(model, train_ds, train_summary_writer):\n",
    "  for epoch in range(EPOCHS):\n",
    "    print(f\"Epoch {epoch}:\")\n",
    "    \n",
    "    for data in tqdm.tqdm(train_ds, position=0, leave=True):\n",
    "        metrics = model.train_step(data)\n",
    "        \n",
    "    with train_summary_writer.as_default():\n",
    "        for metric in model.metrics:\n",
    "            tf.summary.scalar(f\"{metric.name}\", metric.result(), step=epoch)\n",
    "\n",
    "    # print metrics\n",
    "    print(', '.join(map(lambda metric: f\"{metric.name}: {'{:.2f}'.format(metric.result())}\", model.metrics)))\n",
    "\n",
    "    # reset all metrics (requires a reset_metrics method in the model)\n",
    "    model.reset_metrics()    \n",
    "    \n",
    "    # Validation: text generation\n",
    "    prediction = model.generate_text(\"What is\", 15, 3)\n",
    "    print(\"Prediction: \", prediction)\n",
    "\n",
    "    model.reset_metrics()\n",
    "    print(\"\\n\")"
   ],
   "metadata": {
    "id": "p7Fp7VkG1WqB"
   },
   "execution_count": 650,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model = BibleModel(sp_tokenizer)\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "train_log_dir = 'logs/' + current_time + '/train'    \n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "\n",
    "training_loop(model, train_dataset, train_summary_writer)"
   ],
   "metadata": {
    "id": "oY8ztWzO5tvj",
    "outputId": "d3021a05-62fd-45f4-a2d4-999185bc544c",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 834
    }
   },
   "execution_count": 651,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 882/882 [02:59<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 4.96, accuracy: 0.17, top-3-acc: 0.29\n",
      "Prediction:  What is the lord of god and the third part of the\n",
      "\n",
      "\n",
      "Epoch 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 457/882 [01:33<01:26,  4.89it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_11656\\656006860.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[0mtrain_summary_writer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msummary\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcreate_file_writer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtrain_log_dir\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 7\u001B[1;33m \u001B[0mtraining_loop\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtrain_dataset\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtrain_summary_writer\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      8\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_11656\\2285903846.py\u001B[0m in \u001B[0;36mtraining_loop\u001B[1;34m(model, train_ds, train_summary_writer)\u001B[0m\n\u001B[0;32m      3\u001B[0m     \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34mf\"Epoch {epoch}:\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 5\u001B[1;33m     \u001B[1;32mfor\u001B[0m \u001B[0mdata\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mtqdm\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtqdm\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtrain_ds\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mposition\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mleave\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      6\u001B[0m         \u001B[0mmetrics\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtrain_step\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Nextcloud\\Backups\\UNI\\SemesterB\\IANNwTF\\lib\\site-packages\\tqdm\\std.py\u001B[0m in \u001B[0;36m__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1203\u001B[0m                     \u001B[0mdt\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcur_t\u001B[0m \u001B[1;33m-\u001B[0m \u001B[0mlast_print_t\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1204\u001B[0m                     \u001B[1;32mif\u001B[0m \u001B[0mdt\u001B[0m \u001B[1;33m>=\u001B[0m \u001B[0mmininterval\u001B[0m \u001B[1;32mand\u001B[0m \u001B[0mcur_t\u001B[0m \u001B[1;33m>=\u001B[0m \u001B[0mmin_start_t\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1205\u001B[1;33m                         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mupdate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mn\u001B[0m \u001B[1;33m-\u001B[0m \u001B[0mlast_print_n\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1206\u001B[0m                         \u001B[0mlast_print_n\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlast_print_n\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1207\u001B[0m                         \u001B[0mlast_print_t\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlast_print_t\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Nextcloud\\Backups\\UNI\\SemesterB\\IANNwTF\\lib\\site-packages\\tqdm\\std.py\u001B[0m in \u001B[0;36mupdate\u001B[1;34m(self, n)\u001B[0m\n\u001B[0;32m   1254\u001B[0m                     \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_ema_dn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdn\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1255\u001B[0m                     \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_ema_dt\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdt\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1256\u001B[1;33m                 \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrefresh\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlock_args\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlock_args\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1257\u001B[0m                 \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdynamic_miniters\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1258\u001B[0m                     \u001B[1;31m# If no `miniters` was specified, adjust automatically to the\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Nextcloud\\Backups\\UNI\\SemesterB\\IANNwTF\\lib\\site-packages\\tqdm\\std.py\u001B[0m in \u001B[0;36mrefresh\u001B[1;34m(self, nolock, lock_args)\u001B[0m\n\u001B[0;32m   1359\u001B[0m             \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1360\u001B[0m                 \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_lock\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0macquire\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1361\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdisplay\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1362\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mnolock\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1363\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_lock\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrelease\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Nextcloud\\Backups\\UNI\\SemesterB\\IANNwTF\\lib\\site-packages\\tqdm\\std.py\u001B[0m in \u001B[0;36mdisplay\u001B[1;34m(self, msg, pos)\u001B[0m\n\u001B[0;32m   1507\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mpos\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1508\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmoveto\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpos\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1509\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msp\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__str__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mif\u001B[0m \u001B[0mmsg\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m \u001B[1;32melse\u001B[0m \u001B[0mmsg\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1510\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mpos\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1511\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmoveto\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m-\u001B[0m\u001B[0mpos\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Nextcloud\\Backups\\UNI\\SemesterB\\IANNwTF\\lib\\site-packages\\tqdm\\std.py\u001B[0m in \u001B[0;36mprint_status\u001B[1;34m(s)\u001B[0m\n\u001B[0;32m    348\u001B[0m         \u001B[1;32mdef\u001B[0m \u001B[0mprint_status\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0ms\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    349\u001B[0m             \u001B[0mlen_s\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdisp_len\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0ms\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 350\u001B[1;33m             \u001B[0mfp_write\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'\\r'\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0ms\u001B[0m \u001B[1;33m+\u001B[0m \u001B[1;33m(\u001B[0m\u001B[1;34m' '\u001B[0m \u001B[1;33m*\u001B[0m \u001B[0mmax\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlast_len\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m-\u001B[0m \u001B[0mlen_s\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    351\u001B[0m             \u001B[0mlast_len\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mlen_s\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    352\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Nextcloud\\Backups\\UNI\\SemesterB\\IANNwTF\\lib\\site-packages\\tqdm\\std.py\u001B[0m in \u001B[0;36mfp_write\u001B[1;34m(s)\u001B[0m\n\u001B[0;32m    341\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    342\u001B[0m         \u001B[1;32mdef\u001B[0m \u001B[0mfp_write\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0ms\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 343\u001B[1;33m             \u001B[0mfp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwrite\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0m_unicode\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0ms\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    344\u001B[0m             \u001B[0mfp_flush\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    345\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Nextcloud\\Backups\\UNI\\SemesterB\\IANNwTF\\lib\\site-packages\\tqdm\\utils.py\u001B[0m in \u001B[0;36minner\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    143\u001B[0m         \u001B[1;32mdef\u001B[0m \u001B[0minner\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    144\u001B[0m             \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 145\u001B[1;33m                 \u001B[1;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    146\u001B[0m             \u001B[1;32mexcept\u001B[0m \u001B[0mOSError\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    147\u001B[0m                 \u001B[1;32mif\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0merrno\u001B[0m \u001B[1;33m!=\u001B[0m \u001B[1;36m5\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Nextcloud\\Backups\\UNI\\SemesterB\\IANNwTF\\lib\\site-packages\\ipykernel\\iostream.py\u001B[0m in \u001B[0;36mwrite\u001B[1;34m(self, string)\u001B[0m\n\u001B[0;32m    561\u001B[0m                 \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpub_thread\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mschedule\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_flush\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    562\u001B[0m             \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 563\u001B[1;33m                 \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_schedule_flush\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    564\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    565\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mstring\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Nextcloud\\Backups\\UNI\\SemesterB\\IANNwTF\\lib\\site-packages\\ipykernel\\iostream.py\u001B[0m in \u001B[0;36m_schedule_flush\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    467\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_io_loop\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcall_later\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mflush_interval\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_flush\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    468\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 469\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpub_thread\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mschedule\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0m_schedule_in_thread\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    470\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    471\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mflush\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Nextcloud\\Backups\\UNI\\SemesterB\\IANNwTF\\lib\\site-packages\\ipykernel\\iostream.py\u001B[0m in \u001B[0;36mschedule\u001B[1;34m(self, f)\u001B[0m\n\u001B[0;32m    208\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_events\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mf\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    209\u001B[0m             \u001B[1;31m# wake event thread (message content is ignored)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 210\u001B[1;33m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_event_pipe\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34mb\"\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    211\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    212\u001B[0m             \u001B[0mf\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Nextcloud\\Backups\\UNI\\SemesterB\\IANNwTF\\lib\\site-packages\\zmq\\sugar\\socket.py\u001B[0m in \u001B[0;36msend\u001B[1;34m(self, data, flags, copy, track, routing_id, group)\u001B[0m\n\u001B[0;32m    618\u001B[0m                 )\n\u001B[0;32m    619\u001B[0m             \u001B[0mdata\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgroup\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mgroup\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 620\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0msuper\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mflags\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mflags\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcopy\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mcopy\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtrack\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mtrack\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    621\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    622\u001B[0m     def send_multipart(\n",
      "\u001B[1;32mzmq\\backend\\cython\\socket.pyx\u001B[0m in \u001B[0;36mzmq.backend.cython.socket.Socket.send\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;32mzmq\\backend\\cython\\socket.pyx\u001B[0m in \u001B[0;36mzmq.backend.cython.socket.Socket.send\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;32mzmq\\backend\\cython\\socket.pyx\u001B[0m in \u001B[0;36mzmq.backend.cython.socket._send_copy\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;32mD:\\Nextcloud\\Backups\\UNI\\SemesterB\\IANNwTF\\lib\\site-packages\\zmq\\backend\\cython\\checkrc.pxd\u001B[0m in \u001B[0;36mzmq.backend.cython.checkrc._check_rc\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs/"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ki_klausur",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ccc2749a7be4b92b5b584e86e3aa803380c51e1da575f498b6f2d8cfba82568a"
   }
  },
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
