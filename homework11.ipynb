{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "_0m4oIEYlSjr",
    "outputId": "098fe011-1ebb-4d82-a380-530fc30b64eb",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.3/1.3 MB\u001B[0m \u001B[31m16.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.97\n"
     ]
    }
   ],
   "source": [
    "# Install tensorflow_text, if executed in google colab\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "  !pip install -q -U \"tensorflow-text==2.8.*\"\n",
    "  !pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "g6BSKo0jlSjt",
    "outputId": "9cb1d752-bfb1-4a08-8f07-6d384a6ab8f4",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Num GPUs Available:  []\n"
     ]
    }
   ],
   "source": [
    "# disable compiler warnings\n",
    "import os\n",
    "\n",
    "# imports \n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_text as tf_text\n",
    "from tensorflow.python.ops.numpy_ops import np_config\n",
    "np_config.enable_numpy_behavior()\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from typing import List\n",
    "import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "import sentencepiece as sp\n",
    "\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '0'  # FATAL\n",
    "print(\"Num GPUs Available: \", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load data"
   ],
   "metadata": {
    "id": "wX8dmMUooDf3"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "Vo5H3SP8lSjw"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Load file from remote, if notebook is executed inside google colab, otherwise it gets loaded from the local file system\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "  bible_url = \"https://raw.githubusercontent.com/IANNwTF-Group-3/homework11/main/bible.txt\"\n",
    "  response = requests.get(bible_url)\n",
    "  text = response.text\n",
    "else:\n",
    "  file_path = \"bible.txt\"\n",
    "  with open(file_path, \"r\") as f:\n",
    "      text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocessing"
   ],
   "metadata": {
    "id": "WNaI2FJQoPDC"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import re\n",
    "\n",
    "# Lowercase the text\n",
    "text = text.lower()\n",
    "\n",
    "# Remove sentence numeration\n",
    "text = re.sub('[0-9]+:[0-9]+ ', '', text)\n",
    "\n",
    "# Remove special characters\n",
    "for c in \"!'()*,-.0123456789:;?\":\n",
    "  text = text.replace(c, '')\n",
    "\n",
    "# Replace multiple spaces with a single space\n",
    "text = re.sub(' +', ' ', text)\n",
    "\n",
    "sentence_separator = \"sentence-separator-placeholder\"\n",
    "# Remember double line breaks\n",
    "text = re.sub('\\n\\n+', sentence_separator, text)\n",
    "# Remove line breaks\n",
    "text = text.replace('\\n', '')\n",
    "# Substitute sentence line breaks back into text\n",
    "text = text.replace(sentence_separator, '\\n')"
   ],
   "metadata": {
    "id": "5yZAxKWhoQY-"
   },
   "execution_count": 66,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Write text to file for later processing"
   ],
   "metadata": {
    "id": "0xLy8nC1yWBS"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "bible_file_name = \"bible_preprocessed.txt\"\n",
    "\n",
    "if os.path.exists(bible_file_name):\n",
    "  os.remove(bible_file_name)\n",
    "\n",
    "bible_file = open(bible_file_name, 'xb')\n",
    "bible_file.write(text.encode(encoding='UTF-8'))\n",
    "bible_file.close()"
   ],
   "metadata": {
    "id": "0yNcNqj5yVR1"
   },
   "execution_count": 67,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tokenize"
   ],
   "metadata": {
    "id": "UGxyhxRYoTAs"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create tokenizer model"
   ],
   "metadata": {
    "id": "iZc3nOsCy9hd"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Pretrained model\n",
    "# sp_model_url = \"https://github.com/tensorflow/text/blob/master/tensorflow_text/python/ops/test_data/fast_sentencepiece.model?raw=true\"\n",
    "# sp_model = requests.get(sp_model_url).content\n",
    "\n",
    "# Self trained model\n",
    "sp_model_name = \"sp_tokenizer\"\n",
    "sp.SentencePieceTrainer.train(input=bible_file_name, model_prefix=sp_model_name, model_type=\"unigram\", vocab_size=3000)\n",
    "sp_model = tf.io.gfile.GFile(f\"{sp_model_name}.model\", \"rb\").read()"
   ],
   "metadata": {
    "id": "4zHpREzOy5AM"
   },
   "execution_count": 70,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create tokenizer and bible tokens"
   ],
   "metadata": {
    "id": "fntalvOf6q_w"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "sp_tokenizer = tf_text.SentencepieceTokenizer(sp_model)\n",
    "sp_tokens = sp_tokenizer.tokenize(text)"
   ],
   "metadata": {
    "id": "Zs5wtRRSoSs-"
   },
   "execution_count": 75,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Test tokenizer"
   ],
   "metadata": {
    "id": "UC0nRa4b6k3Y"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "test_tokens = sp_tokenizer.tokenize(\"I Jesus have sent mine angel\".lower())\n",
    "for t in test_tokens:\n",
    "  print(sp_tokenizer.detokenize([t]))"
   ],
   "metadata": {
    "id": "n55Zj_Mi6mhE",
    "outputId": "e7e5c6f4-c030-4660-9253-3fb039952011",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "execution_count": 76,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tf.Tensor(b'i', shape=(), dtype=string)\n",
      "tf.Tensor(b'jesus', shape=(), dtype=string)\n",
      "tf.Tensor(b'have', shape=(), dtype=string)\n",
      "tf.Tensor(b'sent', shape=(), dtype=string)\n",
      "tf.Tensor(b'mine', shape=(), dtype=string)\n",
      "tf.Tensor(b'angel', shape=(), dtype=string)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prepare dataset"
   ],
   "metadata": {
    "id": "e35hUUMh_3qk"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create training data using sliding window"
   ],
   "metadata": {
    "id": "8hY31BhZ7g8u"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "SLIDING_WINDOW_SIZE = 64\n",
    "BATCH_SIZE = 64\n",
    "# We now split each sentence. We concatenate each sentence to retrieve a better context\n",
    "words = tf.constant(text.replace('\\n', ' ').split(' '))\n",
    "\n",
    "sliding_window = tf_text.sliding_window(data=words, width=SLIDING_WINDOW_SIZE + 1, axis=0)\n",
    "\n",
    "# Visualize sliding window\n",
    "print(sliding_window)"
   ],
   "metadata": {
    "id": "FimTDaki7mWf",
    "outputId": "431255f5-0536-49e8-c496-f7f204bf4309",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "execution_count": 96,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tf.Tensor(\n",
      "[[b'the' b'first' b'book' ... b'was' b'good' b'and']\n",
      " [b'first' b'book' b'of' ... b'good' b'and' b'god']\n",
      " [b'book' b'of' b'moses' ... b'and' b'god' b'divided']\n",
      " ...\n",
      " [b'any' b'man' b'shall' ... b'be' b'with' b'you']\n",
      " [b'man' b'shall' b'take' ... b'with' b'you' b'all']\n",
      " [b'shall' b'take' b'away' ... b'you' b'all' b'amen']], shape=(740430, 65), dtype=string)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create dataset"
   ],
   "metadata": {
    "id": "egMoAnlq96BD"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((sliding_window[:,:-1], sliding_window[:,-1]))\n",
    "dataset = dataset.shuffle(4096)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.AUTOTUNE)"
   ],
   "metadata": {
    "id": "4Q3d204E95tR"
   },
   "execution_count": 99,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ki_klausur",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ccc2749a7be4b92b5b584e86e3aa803380c51e1da575f498b6f2d8cfba82568a"
   }
  },
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
