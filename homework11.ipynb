{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "id": "_0m4oIEYlSjr"
      },
      "outputs": [],
      "source": [
        "# Install tensorflow_text, if executed in google colab\n",
        "if 'google.colab' in str(get_ipython()):\n",
        "  !pip install -q -U tensorflow --upgrade # We need a newer tensorflow version to use the causal mask of the multi head attention layer\n",
        "  !pip install -q -U tensorflow-text\n",
        "  !pip install -q -U sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "id": "g6BSKo0jlSjt",
        "outputId": "58756bd2-3f41-48b6-d0e9-67d5c608f0ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
            "TensorFlow Version:  2.11.0\n"
          ]
        }
      ],
      "source": [
        "# disable compiler warnings\n",
        "import os\n",
        "\n",
        "# imports \n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_text as tf_text\n",
        "from tensorflow.python.ops.numpy_ops import np_config\n",
        "np_config.enable_numpy_behavior()\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.python.keras.layers import Dense\n",
        "from typing import List\n",
        "import datetime\n",
        "from tqdm.notebook import tqdm\n",
        "import sentencepiece as sp\n",
        "import math\n",
        "import tqdm\n",
        "\n",
        "\n",
        "from tensorflow.python.client import device_lib\n",
        "#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '0'  # FATAL\n",
        "print(\"Num GPUs Available: \", tf.config.list_physical_devices('GPU'))\n",
        "print(\"TensorFlow Version: \", tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Constants / Hyperparameter"
      ],
      "metadata": {
        "collapsed": false,
        "id": "620pm5bqcFaX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "outputs": [],
      "source": [
        "PREPROCESSED_BIBLE_FILE_NAME = \"bible_preprocessed.txt\"\n",
        "VOCABULARY_SIZE = 3000 # 2000 to 7000\n",
        "SEQUENCE_LENGTH = 64 # 32 to 256\n",
        "BATCH_SIZE = 64\n",
        "EMBEDDING_OUT = 128 # 64 to 256\n",
        "ATTENTION_HEADS = 4 # 2 to 4\n",
        "TRANSFORMER_DENSE_SIZE = 128 # 32 to 256\n",
        "EPOCHS = 100 # 100 to 600\n",
        "TRAIN_SPLIT = 0.8\n",
        "TEST_SPLIT = 1 - TRAIN_SPLIT"
      ],
      "metadata": {
        "id": "_RI1GS_YcFaZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load data"
      ],
      "metadata": {
        "id": "wX8dmMUooDf3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {
        "id": "Vo5H3SP8lSjw"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "# Load file from remote, if notebook is executed inside google colab, otherwise it gets loaded from the local file system\n",
        "if 'google.colab' in str(get_ipython()):\n",
        "  bible_url = \"https://raw.githubusercontent.com/IANNwTF-Group-3/homework11/main/bible.txt\"\n",
        "  response = requests.get(bible_url)\n",
        "  text = response.text\n",
        "else:\n",
        "  file_path = \"bible.txt\"\n",
        "  with open(file_path, \"r\") as f:\n",
        "      text = f.read()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "WNaI2FJQoPDC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Lowercase the text\n",
        "text = text.lower()\n",
        "\n",
        "# Remove sentence numeration\n",
        "text = re.sub('[0-9]+:[0-9]+ ', '', text)\n",
        "\n",
        "# Remove special characters\n",
        "for c in \"!'()*,-.0123456789:;?\":\n",
        "  text = text.replace(c, '')\n",
        "\n",
        "# Replace multiple spaces with a single space\n",
        "text = re.sub(' +', ' ', text)\n",
        "\n",
        "sentence_separator = \"sentence-separator-placeholder\"\n",
        "# Remember double line breaks\n",
        "text = re.sub('\\n\\n+', sentence_separator, text)\n",
        "# Remove line breaks\n",
        "text = text.replace('\\n', '')\n",
        "# Substitute sentence line breaks back into text\n",
        "text = text.replace(sentence_separator, '\\n')"
      ],
      "metadata": {
        "id": "5yZAxKWhoQY-"
      },
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write text to file for later processing"
      ],
      "metadata": {
        "id": "0xLy8nC1yWBS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if os.path.exists(PREPROCESSED_BIBLE_FILE_NAME):\n",
        "  os.remove(PREPROCESSED_BIBLE_FILE_NAME)\n",
        "\n",
        "bible_file = open(PREPROCESSED_BIBLE_FILE_NAME, 'xb')\n",
        "bible_file.write(text.encode(encoding='UTF-8'))\n",
        "bible_file.close()"
      ],
      "metadata": {
        "id": "0yNcNqj5yVR1"
      },
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenize"
      ],
      "metadata": {
        "id": "UGxyhxRYoTAs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create tokenizer model"
      ],
      "metadata": {
        "id": "iZc3nOsCy9hd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pretrained model\n",
        "# sp_model_url = \"https://github.com/tensorflow/text/blob/master/tensorflow_text/python/ops/test_data/fast_sentencepiece.model?raw=true\"\n",
        "# sp_model = requests.get(sp_model_url).content\n",
        "\n",
        "# Self trained model\n",
        "sp_model_name = \"sp_tokenizer\"\n",
        "sp.SentencePieceTrainer.train(input=PREPROCESSED_BIBLE_FILE_NAME, model_prefix=sp_model_name, model_type=\"unigram\", vocab_size=VOCABULARY_SIZE)\n",
        "sp_model = tf.io.gfile.GFile(f\"{sp_model_name}.model\", \"rb\").read()"
      ],
      "metadata": {
        "id": "4zHpREzOy5AM"
      },
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create tokenizer and bible tokens"
      ],
      "metadata": {
        "id": "fntalvOf6q_w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sp_tokenizer = tf_text.SentencepieceTokenizer(sp_model)\n",
        "sp_tokens = sp_tokenizer.tokenize(text)"
      ],
      "metadata": {
        "id": "Zs5wtRRSoSs-"
      },
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test tokenizer"
      ],
      "metadata": {
        "id": "UC0nRa4b6k3Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_tokens = sp_tokenizer.tokenize(\"I Jesus have sent mine angel\".lower())\n",
        "for t in test_tokens:\n",
        "  print(sp_tokenizer.detokenize([t]))"
      ],
      "metadata": {
        "id": "n55Zj_Mi6mhE",
        "outputId": "f262ec96-0b85-4ed1-88d0-bd2140e4b420",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(b'i', shape=(), dtype=string)\n",
            "tf.Tensor(b'jesus', shape=(), dtype=string)\n",
            "tf.Tensor(b'have', shape=(), dtype=string)\n",
            "tf.Tensor(b'sent', shape=(), dtype=string)\n",
            "tf.Tensor(b'mine', shape=(), dtype=string)\n",
            "tf.Tensor(b'angel', shape=(), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare dataset"
      ],
      "metadata": {
        "id": "e35hUUMh_3qk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create training data using sliding window"
      ],
      "metadata": {
        "id": "8hY31BhZ7g8u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sliding_window = tf_text.sliding_window(data=sp_tokens, width=SEQUENCE_LENGTH + 1, axis=0)\n",
        "\n",
        "# Visualize sliding window\n",
        "print(sliding_window)"
      ],
      "metadata": {
        "id": "FimTDaki7mWf",
        "outputId": "65f5fed3-a348-4792-9604-96fdcaa02024",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[   3  292  562 ...    4   32  237]\n",
            " [ 292  562    5 ...   32  237    3]\n",
            " [ 562    5  172 ...  237    3  392]\n",
            " ...\n",
            " [ 562    5   51 ...   19   26   57]\n",
            " [   5   51 2700 ...   26   57   28]\n",
            " [  51 2700   32 ...   57   28 1339]], shape=(902189, 65), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create dataset"
      ],
      "metadata": {
        "id": "egMoAnlq96BD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "full_dataset = tf.data.Dataset.from_tensor_slices((sliding_window[:,:-1], sliding_window[:,1:]))\n",
        "#full_dataset = tf.data.Dataset.from_tensor_slices(sliding_window)\n",
        "full_dataset = full_dataset.shuffle(4096)\n",
        "full_dataset = full_dataset.batch(BATCH_SIZE)\n",
        "full_dataset = full_dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "train_size = math.floor(len(full_dataset) * TRAIN_SPLIT)\n",
        "test_size = math.floor(len(full_dataset) * TEST_SPLIT)\n",
        "train_dataset = full_dataset.take(train_size)\n",
        "test_dataset = full_dataset.skip(train_size).take(test_size)"
      ],
      "metadata": {
        "id": "4Q3d204E95tR"
      },
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embedding layer"
      ],
      "metadata": {
        "collapsed": false,
        "id": "uNqwRZK4cFao"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "outputs": [],
      "source": [
        "class BibleEmbedding(tf.keras.layers.Layer):\n",
        "  def __init__(self):\n",
        "    super(BibleEmbedding, self).__init__()\n",
        "\n",
        "    self.token_embedding = tf.keras.layers.Embedding(VOCABULARY_SIZE, EMBEDDING_OUT)\n",
        "    self.position_embedding = tf.keras.layers.Embedding(SEQUENCE_LENGTH, EMBEDDING_OUT)\n",
        "\n",
        "  def call(self, input_sequence):\n",
        "    print(input_sequence.shape)\n",
        "    input_range = tf.range(0, input_sequence.shape[1])\n",
        "\n",
        "    return tf.math.add(self.token_embedding(input_sequence), self.position_embedding(input_range))"
      ],
      "metadata": {
        "id": "KPSVQkHkcFao"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer block"
      ],
      "metadata": {
        "collapsed": false,
        "id": "myTb9RNdcFap"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "outputs": [],
      "source": [
        "class TransformerBlock(tf.keras.layers.Layer):\n",
        "  def __init__(self):\n",
        "    super(TransformerBlock, self).__init__()\n",
        "    self.head_attention_layer = tf.keras.layers.MultiHeadAttention(ATTENTION_HEADS, EMBEDDING_OUT)\n",
        "    self.dense1 = tf.keras.layers.Dense(TRANSFORMER_DENSE_SIZE, activation='relu')\n",
        "    self.dense2 = tf.keras.layers.Dense(EMBEDDING_OUT, activation=None)\n",
        "    self.dropout1 = tf.keras.layers.Dropout(0.1)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(0.1)\n",
        "    self.layer_normalization1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layer_normalization2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "  def call(self, data, training):\n",
        "    x = self.head_attention_layer(data, data, use_causal_mask=True)\n",
        "    x = self.dropout1(x, training=training)\n",
        "    x = tf.math.add(x, data)\n",
        "    x = self.layer_normalization1(x)\n",
        "    y = self.dense1(x)\n",
        "    y = self.dense2(y)\n",
        "    y = self.dropout2(y)\n",
        "    x = tf.math.add(x, y)\n",
        "    return self.layer_normalization2(x)"
      ],
      "metadata": {
        "id": "IWV6ajI5cFap"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bible model"
      ],
      "metadata": {
        "collapsed": false,
        "id": "U1p78otwcFap"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "outputs": [],
      "source": [
        "class BibleModel(tf.keras.Model):\n",
        "  def __init__(self, tokenizer, optimizer=tf.keras.optimizers.Adam(), loss_function=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)):\n",
        "    super(BibleModel, self).__init__()\n",
        "    self.tokenizer = tokenizer\n",
        "    self.optimizer = optimizer\n",
        "    self.loss_function = loss_function\n",
        "    self.metrics_list = [\n",
        "      tf.keras.metrics.Mean(name=\"loss\"),\n",
        "      tf.keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n",
        "      #tf.keras.metrics.TopKCategoricalAccuracy(3,name=\"top-3-acc\")\n",
        "    ]\n",
        "\n",
        "    self.embedding_layer = BibleEmbedding()\n",
        "    self.transformer_layer = TransformerBlock()\n",
        "    self.out_layer = tf.keras.layers.Dense(VOCABULARY_SIZE, activation=None)\n",
        "\n",
        "  def call(self, data, training=True):\n",
        "    x = self.embedding_layer(data)\n",
        "    x = self.transformer_layer(x)\n",
        "    return self.out_layer(x)\n",
        "\n",
        "  def reset_metrics(self):\n",
        "    for metric in self.metrics:\n",
        "        metric.reset_states()\n",
        "\n",
        "  @tf.function\n",
        "  def train_step(self, data):\n",
        "    x, targets = data\n",
        "      \n",
        "    # compute output and loss, train the variables\n",
        "    with tf.GradientTape() as tape:\n",
        "      predictions = self(x, training=True)\n",
        "      loss = self.loss_function(targets, predictions) + tf.reduce_sum(self.losses)\n",
        "          \n",
        "    # update trainable variables\n",
        "    gradients = tape.gradient(loss, self.trainable_variables)\n",
        "    self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
        "\n",
        "    # update metrics\n",
        "    self.metrics_list[0].update_state(loss)\n",
        "\n",
        "    for metric in self.metrics_list[1:]:\n",
        "        metric.update_state(targets, predictions)\n",
        "      \n",
        "    # return a dict with metric information\n",
        "    return {m.name : m.result() for m in self.metrics_list}\n",
        "\n",
        "  def generate_text(self, prompt, output_length, top_k):\n",
        "    generated = self.tokenizer.detokenize(self.tokenizer.tokenize(prompt))\n",
        "    while len(generated) < len(prompt) + output_length:\n",
        "        tokenized_prompt = self.tokenizer(generated)\n",
        "        output = self(tf.expand_dims(tokenized_prompt, 0), training=False)\n",
        "        logits = output[:, -1, :]  # select the last token's logits\n",
        "        filtered_logits, top_indices = tf.math.top_k(logits, k=top_k, sorted=True)\n",
        "        chosen_index = tf.random.categorical(filtered_logits, num_samples=1)[-1, 0].numpy()\n",
        "        generated += self.tokenizer.detokenize([top_indices[0][chosen_index].numpy()])\n",
        "        tokenized_prompt = self.tokenizer(generated)[-self.max_len:]\n",
        "    return ' '.join(self.tokenizer.detokenize(self.tokenizer.tokenize(generated)[len(prompt):]))\n",
        "\n"
      ],
      "metadata": {
        "id": "fCpw7kfXcFap"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training loop"
      ],
      "metadata": {
        "id": "kOAv_LQM1VKc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def training_loop(model, train_ds, train_summary_writer):\n",
        "  for epoch in range(EPOCHS):\n",
        "    print(f\"Epoch {epoch}:\")\n",
        "    \n",
        "    for data in tqdm.tqdm(train_ds, position=0, leave=True):\n",
        "        metrics = model.train_step(data)\n",
        "        \n",
        "    with train_summary_writer.as_default():\n",
        "        for metric in model.metrics:\n",
        "            tf.summary.scalar(f\"{metric.name}\", metric.result(), step=epoch)\n",
        "\n",
        "    # print metrics\n",
        "    for metric in model.metrics:\n",
        "      print(f\"{metric.name}: {metric.result()}\")\n",
        "\n",
        "    # reset all metrics (requires a reset_metrics method in the model)\n",
        "    model.reset_metrics()    \n",
        "    \n",
        "    # Validation: text generation\n",
        "    #prediction = model.generate_text(\"What is\", 10, 3)\n",
        "    #print(\"Prediction: \", prediction)\n",
        "\n",
        "    model.reset_metrics()\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "id": "p7Fp7VkG1WqB"
      },
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BibleModel(sp_tokenizer)\n",
        "\n",
        "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "train_log_dir = 'logs/' + current_time + '/train'    \n",
        "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
        "\n",
        "training_loop(model, train_dataset, train_summary_writer)\n"
      ],
      "metadata": {
        "id": "oY8ztWzO5tvj",
        "outputId": "d3021a05-62fd-45f4-a2d4-999185bc544c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 834
        }
      },
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/11277 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 64)\n",
            "(64, 64)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11277/11277 [03:37<00:00, 51.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 2.345102071762085\n",
            "accuracy: 0.00946571584790945\n",
            "\n",
            "\n",
            "Epoch 1:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11277/11277 [02:51<00:00, 65.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 2.6411609649658203\n",
            "accuracy: 0.009846053086221218\n",
            "\n",
            "\n",
            "Epoch 2:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11277/11277 [02:53<00:00, 65.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 2.844372034072876\n",
            "accuracy: 0.010191361419856548\n",
            "\n",
            "\n",
            "Epoch 3:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11277/11277 [02:57<00:00, 63.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 2.9216980934143066\n",
            "accuracy: 0.010348665527999401\n",
            "\n",
            "\n",
            "Epoch 4:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  4%|▍         | 433/11277 [00:07<02:57, 61.20it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-167-00be34ba226e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain_summary_writer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_file_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_log_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtraining_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_summary_writer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-166-87b65e51f8fb>\u001b[0m in \u001b[0;36mtraining_loop\u001b[0;34m(model, train_ds, train_summary_writer)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrain_summary_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    910\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m       (concrete_function,\n\u001b[1;32m    133\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m--> 134\u001b[0;31m     return concrete_function._call_flat(\n\u001b[0m\u001b[1;32m    135\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1743\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1745\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1746\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    376\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    379\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ki_klausur",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "ccc2749a7be4b92b5b584e86e3aa803380c51e1da575f498b6f2d8cfba82568a"
      }
    },
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}