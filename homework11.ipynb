{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "_0m4oIEYlSjr"
   },
   "outputs": [],
   "source": [
    "# Install tensorflow_text, if executed in google colab\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "  !pip install -q -U tensorflow --upgrade # We need a newer tensorflow version to use the causal mask of the multi head attention layer\n",
    "  !pip install -q -U tensorflow-text\n",
    "  !pip install -q -U sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "g6BSKo0jlSjt",
    "outputId": "c96b7323-f587-4460-ef8e-e945ce5b5509",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Num GPUs Available:  []\n",
      "TensorFlow Version:  2.11.0\n"
     ]
    }
   ],
   "source": [
    "# disable compiler warnings\n",
    "import os\n",
    "\n",
    "# imports \n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_text as tf_text\n",
    "from tensorflow.python.ops.numpy_ops import np_config\n",
    "np_config.enable_numpy_behavior()\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from typing import List\n",
    "import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "import sentencepiece as sp\n",
    "import math\n",
    "\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '0'  # FATAL\n",
    "print(\"Num GPUs Available: \", tf.config.list_physical_devices('GPU'))\n",
    "print(\"TensorFlow Version: \", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Constants / Hyperparameter"
   ],
   "metadata": {
    "collapsed": false,
    "id": "620pm5bqcFaX"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "PREPROCESSED_BIBLE_FILE_NAME = \"bible_preprocessed.txt\"\n",
    "VOCABULARY_SIZE = 3000 # 2000 to 7000\n",
    "SEQUENCE_LENGTH = 64 # 32 to 256\n",
    "BATCH_SIZE = 64\n",
    "EMBEDDING_OUT = 128 # 64 to 256\n",
    "ATTENTION_HEADS = 4 # 2 to 4\n",
    "TRANSFORMER_DENSE_SIZE = 128 # 32 to 256\n",
    "EPOCHS = 100 # 100 to 600\n",
    "TRAIN_SPLIT = 0.8\n",
    "TEST_SPLIT = 1 - TRAIN_SPLIT"
   ],
   "metadata": {
    "id": "_RI1GS_YcFaZ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load data"
   ],
   "metadata": {
    "id": "wX8dmMUooDf3"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Vo5H3SP8lSjw"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Load file from remote, if notebook is executed inside google colab, otherwise it gets loaded from the local file system\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "  bible_url = \"https://raw.githubusercontent.com/IANNwTF-Group-3/homework11/main/bible.txt\"\n",
    "  response = requests.get(bible_url)\n",
    "  text = response.text\n",
    "else:\n",
    "  file_path = \"bible.txt\"\n",
    "  with open(file_path, \"r\") as f:\n",
    "      text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocessing"
   ],
   "metadata": {
    "id": "WNaI2FJQoPDC"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import re\n",
    "\n",
    "# Lowercase the text\n",
    "text = text.lower()\n",
    "\n",
    "# Remove sentence numeration\n",
    "text = re.sub('[0-9]+:[0-9]+ ', '', text)\n",
    "\n",
    "# Remove special characters\n",
    "for c in \"!'()*,-.0123456789:;?\":\n",
    "  text = text.replace(c, '')\n",
    "\n",
    "# Replace multiple spaces with a single space\n",
    "text = re.sub(' +', ' ', text)\n",
    "\n",
    "sentence_separator = \"sentence-separator-placeholder\"\n",
    "# Remember double line breaks\n",
    "text = re.sub('\\n\\n+', sentence_separator, text)\n",
    "# Remove line breaks\n",
    "text = text.replace('\\n', '')\n",
    "# Substitute sentence line breaks back into text\n",
    "text = text.replace(sentence_separator, '\\n')"
   ],
   "metadata": {
    "id": "5yZAxKWhoQY-"
   },
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Write text to file for later processing"
   ],
   "metadata": {
    "id": "0xLy8nC1yWBS"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "if os.path.exists(PREPROCESSED_BIBLE_FILE_NAME):\n",
    "  os.remove(PREPROCESSED_BIBLE_FILE_NAME)\n",
    "\n",
    "bible_file = open(PREPROCESSED_BIBLE_FILE_NAME, 'xb')\n",
    "bible_file.write(text.encode(encoding='UTF-8'))\n",
    "bible_file.close()"
   ],
   "metadata": {
    "id": "0yNcNqj5yVR1"
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tokenize"
   ],
   "metadata": {
    "id": "UGxyhxRYoTAs"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create tokenizer model"
   ],
   "metadata": {
    "id": "iZc3nOsCy9hd"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Pretrained model\n",
    "# sp_model_url = \"https://github.com/tensorflow/text/blob/master/tensorflow_text/python/ops/test_data/fast_sentencepiece.model?raw=true\"\n",
    "# sp_model = requests.get(sp_model_url).content\n",
    "\n",
    "# Self trained model\n",
    "sp_model_name = \"sp_tokenizer\"\n",
    "sp.SentencePieceTrainer.train(input=PREPROCESSED_BIBLE_FILE_NAME, model_prefix=sp_model_name, model_type=\"unigram\", vocab_size=VOCABULARY_SIZE)\n",
    "sp_model = tf.io.gfile.GFile(f\"{sp_model_name}.model\", \"rb\").read()"
   ],
   "metadata": {
    "id": "4zHpREzOy5AM"
   },
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create tokenizer and bible tokens"
   ],
   "metadata": {
    "id": "fntalvOf6q_w"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "sp_tokenizer = tf_text.SentencepieceTokenizer(sp_model)\n",
    "sp_tokens = sp_tokenizer.tokenize(text)"
   ],
   "metadata": {
    "id": "Zs5wtRRSoSs-",
    "outputId": "3f8fcf43-6fe9-4efa-822c-acdcd4165476",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "execution_count": 29,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tf.Tensor([   3  292  562 ...   57   28 1339], shape=(902253,), dtype=int32)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Test tokenizer"
   ],
   "metadata": {
    "id": "UC0nRa4b6k3Y"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "test_tokens = sp_tokenizer.tokenize(\"I Jesus have sent mine angel\".lower())\n",
    "for t in test_tokens:\n",
    "  print(sp_tokenizer.detokenize([t]))"
   ],
   "metadata": {
    "id": "n55Zj_Mi6mhE",
    "outputId": "34a02b34-b0e3-4e75-b36d-841a113f4545",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "execution_count": 15,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tf.Tensor(b'i', shape=(), dtype=string)\n",
      "tf.Tensor(b'jesus', shape=(), dtype=string)\n",
      "tf.Tensor(b'have', shape=(), dtype=string)\n",
      "tf.Tensor(b'sent', shape=(), dtype=string)\n",
      "tf.Tensor(b'mine', shape=(), dtype=string)\n",
      "tf.Tensor(b'angel', shape=(), dtype=string)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prepare dataset"
   ],
   "metadata": {
    "id": "e35hUUMh_3qk"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create training data using sliding window"
   ],
   "metadata": {
    "id": "8hY31BhZ7g8u"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "sliding_window = tf_text.sliding_window(data=sp_tokens, width=SEQUENCE_LENGTH + 1, axis=0)\n",
    "\n",
    "# Visualize sliding window\n",
    "print(sliding_window)"
   ],
   "metadata": {
    "id": "FimTDaki7mWf",
    "outputId": "0c75e1af-46f9-4a3a-c797-72189b72e24a",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "execution_count": 31,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tf.Tensor(\n",
      "[[   3  292  562 ...    4   32  237]\n",
      " [ 292  562    5 ...   32  237    3]\n",
      " [ 562    5  172 ...  237    3  392]\n",
      " ...\n",
      " [ 562    5   51 ...   19   26   57]\n",
      " [   5   51 2700 ...   26   57   28]\n",
      " [  51 2700   32 ...   57   28 1339]], shape=(902189, 65), dtype=int32)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create dataset"
   ],
   "metadata": {
    "id": "egMoAnlq96BD"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "full_dataset = tf.data.Dataset.from_tensor_slices((sliding_window[:,:-1], sliding_window[:,1:]))\n",
    "#full_dataset = tf.data.Dataset.from_tensor_slices(sliding_window)\n",
    "full_dataset = full_dataset.shuffle(4096)\n",
    "full_dataset = full_dataset.batch(BATCH_SIZE)\n",
    "full_dataset = full_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "train_size = math.floor(len(full_dataset) * TRAIN_SPLIT)\n",
    "test_size = math.floor(len(full_dataset) * TEST_SPLIT)\n",
    "train_dataset = full_dataset.take(train_size)\n",
    "test_dataset = full_dataset.skip(train_size).take(test_size)"
   ],
   "metadata": {
    "id": "4Q3d204E95tR"
   },
   "execution_count": 32,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Embedding layer"
   ],
   "metadata": {
    "collapsed": false,
    "id": "uNqwRZK4cFao"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "class BibleEmbedding(tf.keras.layers.Layer):\n",
    "  def __init__(self):\n",
    "    super(BibleEmbedding, self).__init__()\n",
    "\n",
    "    self.token_embedding = tf.keras.layers.Embedding(VOCABULARY_SIZE, EMBEDDING_OUT)\n",
    "    self.position_embedding = tf.keras.layers.Embedding(SEQUENCE_LENGTH, EMBEDDING_OUT)\n",
    "\n",
    "  def call(self, input_sequence):\n",
    "    print(input_sequence.shape)\n",
    "    input_range = tf.range(0, input_sequence.shape[1])\n",
    "\n",
    "    return tf.math.add(self.token_embedding(input_sequence), self.position_embedding(input_range))"
   ],
   "metadata": {
    "id": "KPSVQkHkcFao"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Transformer block"
   ],
   "metadata": {
    "collapsed": false,
    "id": "myTb9RNdcFap"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "class TransformerBlock(tf.keras.layers.Layer):\n",
    "  def __init__(self):\n",
    "    super(TransformerBlock, self).__init__()\n",
    "    self.head_attention_layer = tf.keras.layers.MultiHeadAttention(ATTENTION_HEADS, EMBEDDING_OUT)\n",
    "    self.dense1 = tf.keras.layers.Dense(TRANSFORMER_DENSE_SIZE, activation='relu')\n",
    "    self.dense2 = tf.keras.layers.Dense(EMBEDDING_OUT, activation=None)\n",
    "    self.dropout1 = tf.keras.layers.Dropout(0.1)\n",
    "    self.dropout2 = tf.keras.layers.Dropout(0.1)\n",
    "    self.layer_normalization1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layer_normalization2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "  def call(self, data, training):\n",
    "    x = self.head_attention_layer(data, data, use_causal_mask=True)\n",
    "    x = self.dropout1(x, training=training)\n",
    "    x = tf.math.add(x, data)\n",
    "    x = self.layer_normalization1(x)\n",
    "    y = self.dense1(x)\n",
    "    y = self.dense2(y)\n",
    "    y = self.dropout2(y)\n",
    "    x = tf.math.add(x, y)\n",
    "    return self.layer_normalization2(x)"
   ],
   "metadata": {
    "id": "IWV6ajI5cFap"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Bible model"
   ],
   "metadata": {
    "collapsed": false,
    "id": "U1p78otwcFap"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "class BibleModel(tf.keras.Model):\n",
    "  def __init__(self, tokenizer, optimizer=tf.keras.optimizers.Adam(), loss_function=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)):\n",
    "    super(BibleModel, self).__init__()\n",
    "    self.tokenizer = tokenizer\n",
    "    self.optimizer = optimizer\n",
    "    self.loss_function = loss_function\n",
    "    self.metrics_list = [\n",
    "      tf.keras.metrics.Mean(name=\"loss\"),\n",
    "      tf.keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n",
    "      #tf.keras.metrics.TopKCategoricalAccuracy(3,name=\"top-3-acc\")\n",
    "    ]\n",
    "\n",
    "    self.embedding_layer = BibleEmbedding()\n",
    "    self.transformer_layer = TransformerBlock()\n",
    "    self.out_layer = tf.keras.layers.Dense(VOCABULARY_SIZE, activation=None)\n",
    "\n",
    "  def call(self, data, training=True):\n",
    "    x = self.embedding_layer(data)\n",
    "    x = self.transformer_layer(x)\n",
    "    return self.out_layer(x)\n",
    "\n",
    "  def reset_metrics(self):\n",
    "    for metric in self.metrics:\n",
    "        metric.reset_states()\n",
    "\n",
    "  @tf.function\n",
    "  def train_step(self, data):\n",
    "      x, targets = data\n",
    "\n",
    "      with tf.GradientTape() as tape:\n",
    "          predictions = self(x, training=True)\n",
    "\n",
    "          loss = self.loss_function(targets, predictions) + tf.reduce_sum(self.losses)\n",
    "\n",
    "      #gradients = tape.gradient(loss, self.trainable_variables)\n",
    "      #self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "\n",
    "      # update loss metric\n",
    "      self.metrics[0].update_state(loss)\n",
    "\n",
    "      # for all metrics except loss, update states (accuracy etc.)\n",
    "      for metric in self.metrics[1:]:\n",
    "          metric.update_state(targets, predictions)\n",
    "\n",
    "      # Return a dictionary mapping metric names to current value\n",
    "      return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "  def generate_text(self, prompt, output_length, top_k):\n",
    "    # TODO tf.random.categorical & tf.pad\n",
    "    generated = self.tokenizer.detokenize(self.tokenizer.tokenize(prompt))\n",
    "    while len(generated) < len(prompt) + output_length:\n",
    "      tokenized_prompt = self.tokenizer(generated)\n",
    "      output = self(tf.expand_dim(tokenized_prompt, -1), training=False)\n",
    "      print(\"Output: \", output)\n",
    "      tokenized_prompt.push_back(output)\n",
    "      generated = self.tokenizer.detokenize(tokenized_prompt)\n",
    "    return ' '.join(self.tokenizer.detokenize(generated[len(prompt):]))"
   ],
   "metadata": {
    "id": "fCpw7kfXcFap"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training loop"
   ],
   "metadata": {
    "id": "kOAv_LQM1VKc"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def training_loop(model, dataset, train_summary_writer):\n",
    "  epoch_losses = []\n",
    "\n",
    "  for epoch in range(EPOCHS):\n",
    "      print('Epoch: ', epoch)\n",
    "      train_losses = []  # each entry is averaged loss of each batch\n",
    "      # train over all batches\n",
    "\n",
    "      for input_batch in dataset:\n",
    "          train_losses.append(model.train_step(input_batch))\n",
    "\n",
    "      # log train loss\n",
    "      with train_summary_writer.as_default():  \n",
    "          tf.summary.scalar('loss', np.mean(train_losses), step=epoch)\n",
    "      epoch_losses.append(np.mean(train_losses))\n",
    "\n",
    "      # Test model\n",
    "      prediction = model.generate_text(\"What is\", 10, 3)\n",
    "      print(\"Prediction: \", prediction)"
   ],
   "metadata": {
    "id": "p7Fp7VkG1WqB"
   },
   "execution_count": 37,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train the model"
   ],
   "metadata": {
    "id": "XtKBqzog5sRP"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "model = BibleModel(sp_tokenizer)\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "train_log_dir = 'logs/' + current_time + '/train'    \n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "\n",
    "training_loop(model, train_dataset, train_summary_writer)"
   ],
   "metadata": {
    "id": "oY8ztWzO5tvj",
    "outputId": "4149b2b7-5b60-4c84-f777-147634f008de",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch:  0\n",
      "(64, 64)\n",
      "(64, 64)\n"
     ]
    }
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ki_klausur",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ccc2749a7be4b92b5b584e86e3aa803380c51e1da575f498b6f2d8cfba82568a"
   }
  },
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
