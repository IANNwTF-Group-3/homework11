{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "_0m4oIEYlSjr"
   },
   "outputs": [],
   "source": [
    "# Install tensorflow_text, if executed in google colab\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "  !pip install -q -U tensorflow --upgrade # We need a newer tensorflow version to use the causal mask of the multi head attention layer\n",
    "  !pip install -q -U tensorflow-text\n",
    "  !pip install -q -U sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "id": "g6BSKo0jlSjt",
    "outputId": "58756bd2-3f41-48b6-d0e9-67d5c608f0ac",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n",
      "Num GPUs Available:  [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "TensorFlow Version:  2.10.0\n"
     ]
    }
   ],
   "source": [
    "# disable compiler warnings\n",
    "import os\n",
    "\n",
    "# imports \n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_text as tf_text\n",
    "from tensorflow.python.ops.numpy_ops import np_config\n",
    "np_config.enable_numpy_behavior()\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from typing import List\n",
    "import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "import sentencepiece as sp\n",
    "import math\n",
    "import tqdm\n",
    "\n",
    "%load_ext tensorboard\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '0'  # FATAL\n",
    "print(\"Num GPUs Available: \", tf.config.list_physical_devices('GPU'))\n",
    "print(\"TensorFlow Version: \", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Constants / Hyperparameter"
   ],
   "metadata": {
    "collapsed": false,
    "id": "620pm5bqcFaX"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [],
   "source": [
    "PREPROCESSED_BIBLE_FILE_NAME = \"bible_preprocessed.txt\"\n",
    "VOCABULARY_SIZE = 3000 # 2000 to 7000\n",
    "SEQUENCE_LENGTH = 64 # 32 to 256\n",
    "BATCH_SIZE = 1024\n",
    "EMBEDDING_OUT = 64 # 64 to 256\n",
    "ATTENTION_HEADS = 4 # 2 to 4\n",
    "TRANSFORMER_DENSE_SIZE = 128 # 32 to 256\n",
    "EPOCHS = 10 # 100 to 600\n",
    "TRAIN_SPLIT = 1.0"
   ],
   "metadata": {
    "id": "_RI1GS_YcFaZ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load data"
   ],
   "metadata": {
    "id": "wX8dmMUooDf3"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "id": "Vo5H3SP8lSjw"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Load file from remote, if notebook is executed inside google colab, otherwise it gets loaded from the local file system\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "  bible_url = \"https://raw.githubusercontent.com/IANNwTF-Group-3/homework11/main/bible.txt\"\n",
    "  response = requests.get(bible_url)\n",
    "  text = response.text\n",
    "else:\n",
    "  file_path = \"bible.txt\"\n",
    "  with open(file_path, \"r\") as f:\n",
    "      text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocessing"
   ],
   "metadata": {
    "id": "WNaI2FJQoPDC"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import re\n",
    "\n",
    "# Lowercase the text\n",
    "text = text.lower()\n",
    "\n",
    "# Remove sentence numeration\n",
    "text = re.sub('[0-9]+:[0-9]+ ', '', text)\n",
    "\n",
    "# Remove special characters\n",
    "for c in \"!'()*,-.0123456789:;?\":\n",
    "  text = text.replace(c, '')\n",
    "\n",
    "# Replace multiple spaces with a single space\n",
    "text = re.sub(' +', ' ', text)\n",
    "\n",
    "sentence_separator = \"sentence-separator-placeholder\"\n",
    "# Remember double line breaks\n",
    "text = re.sub('\\n\\n+', sentence_separator, text)\n",
    "# Remove line breaks\n",
    "text = text.replace('\\n', '')\n",
    "# Substitute sentence line breaks back into text\n",
    "text = text.replace(sentence_separator, '\\n')"
   ],
   "metadata": {
    "id": "5yZAxKWhoQY-"
   },
   "execution_count": 101,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Write text to file for later processing"
   ],
   "metadata": {
    "id": "0xLy8nC1yWBS"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "if os.path.exists(PREPROCESSED_BIBLE_FILE_NAME):\n",
    "  os.remove(PREPROCESSED_BIBLE_FILE_NAME)\n",
    "\n",
    "bible_file = open(PREPROCESSED_BIBLE_FILE_NAME, 'xb')\n",
    "bible_file.write(text.encode(encoding='UTF-8'))\n",
    "bible_file.close()"
   ],
   "metadata": {
    "id": "0yNcNqj5yVR1"
   },
   "execution_count": 102,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tokenize"
   ],
   "metadata": {
    "id": "UGxyhxRYoTAs"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create tokenizer model"
   ],
   "metadata": {
    "id": "iZc3nOsCy9hd"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Pretrained model\n",
    "# sp_model_url = \"https://github.com/tensorflow/text/blob/master/tensorflow_text/python/ops/test_data/fast_sentencepiece.model?raw=true\"\n",
    "# sp_model = requests.get(sp_model_url).content\n",
    "\n",
    "# Self trained model\n",
    "sp_model_name = \"sp_tokenizer\"\n",
    "sp.SentencePieceTrainer.train(input=PREPROCESSED_BIBLE_FILE_NAME, model_prefix=sp_model_name, model_type=\"unigram\", vocab_size=VOCABULARY_SIZE)\n",
    "sp_model = tf.io.gfile.GFile(f\"{sp_model_name}.model\", \"rb\").read()"
   ],
   "metadata": {
    "id": "4zHpREzOy5AM"
   },
   "execution_count": 103,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create tokenizer and bible tokens"
   ],
   "metadata": {
    "id": "fntalvOf6q_w"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "sp_tokenizer = tf_text.SentencepieceTokenizer(sp_model)\n",
    "sp_tokens = sp_tokenizer.tokenize(text)"
   ],
   "metadata": {
    "id": "Zs5wtRRSoSs-"
   },
   "execution_count": 104,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Test tokenizer"
   ],
   "metadata": {
    "id": "UC0nRa4b6k3Y"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "test_tokens = sp_tokenizer.tokenize(\"I Jesus have sent mine angel\".lower())\n",
    "for t in test_tokens:\n",
    "  print(sp_tokenizer.detokenize([t]))"
   ],
   "metadata": {
    "id": "n55Zj_Mi6mhE",
    "outputId": "f262ec96-0b85-4ed1-88d0-bd2140e4b420",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "execution_count": 105,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'i', shape=(), dtype=string)\n",
      "tf.Tensor(b'jesus', shape=(), dtype=string)\n",
      "tf.Tensor(b'have', shape=(), dtype=string)\n",
      "tf.Tensor(b'sent', shape=(), dtype=string)\n",
      "tf.Tensor(b'mine', shape=(), dtype=string)\n",
      "tf.Tensor(b'angel', shape=(), dtype=string)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prepare dataset"
   ],
   "metadata": {
    "id": "e35hUUMh_3qk"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create training data using sliding window"
   ],
   "metadata": {
    "id": "8hY31BhZ7g8u"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "sliding_window = tf_text.sliding_window(data=sp_tokens, width=SEQUENCE_LENGTH + 1, axis=0)\n",
    "\n",
    "# Visualize sliding window\n",
    "print(sliding_window)"
   ],
   "metadata": {
    "id": "FimTDaki7mWf",
    "outputId": "65f5fed3-a348-4792-9604-96fdcaa02024",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "execution_count": 106,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[   3  292  562 ...    4   32  237]\n",
      " [ 292  562    5 ...   32  237    3]\n",
      " [ 562    5  172 ...  237    3  392]\n",
      " ...\n",
      " [ 562    5   51 ...   19   26   57]\n",
      " [   5   51 2700 ...   26   57   28]\n",
      " [  51 2700   32 ...   57   28 1339]], shape=(902189, 65), dtype=int32)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create dataset"
   ],
   "metadata": {
    "id": "egMoAnlq96BD"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "full_dataset = tf.data.Dataset.from_tensor_slices((sliding_window[:,:-1], sliding_window[:,1:]))\n",
    "full_dataset = full_dataset.cache()\n",
    "full_dataset = full_dataset.shuffle(4096)\n",
    "full_dataset = full_dataset.batch(BATCH_SIZE)\n",
    "full_dataset = full_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "train_size = math.floor(len(full_dataset) * TRAIN_SPLIT)\n",
    "train_dataset = full_dataset.take(train_size)"
   ],
   "metadata": {
    "id": "4Q3d204E95tR"
   },
   "execution_count": 107,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Embedding layer"
   ],
   "metadata": {
    "collapsed": false,
    "id": "uNqwRZK4cFao"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [],
   "source": [
    "class BibleEmbedding(tf.keras.layers.Layer):\n",
    "  def __init__(self):\n",
    "    super(BibleEmbedding, self).__init__()\n",
    "\n",
    "    self.token_embedding = tf.keras.layers.Embedding(VOCABULARY_SIZE, EMBEDDING_OUT)\n",
    "    self.position_embedding = tf.keras.layers.Embedding(SEQUENCE_LENGTH, EMBEDDING_OUT)\n",
    "\n",
    "  def call(self, input_sequence):\n",
    "    input_range = tf.range(0, input_sequence.shape[1])\n",
    "\n",
    "    return tf.math.add(self.token_embedding(input_sequence), self.position_embedding(input_range))"
   ],
   "metadata": {
    "id": "KPSVQkHkcFao"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Transformer block"
   ],
   "metadata": {
    "collapsed": false,
    "id": "myTb9RNdcFap"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [],
   "source": [
    "class TransformerBlock(tf.keras.layers.Layer):\n",
    "  def __init__(self):\n",
    "    super(TransformerBlock, self).__init__()\n",
    "    self.head_attention_layer = tf.keras.layers.MultiHeadAttention(ATTENTION_HEADS, EMBEDDING_OUT)\n",
    "    self.dense = tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(TRANSFORMER_DENSE_SIZE, activation='relu'),\n",
    "      tf.keras.layers.Dense(EMBEDDING_OUT, activation=None)\n",
    "    ])\n",
    "    self.dropout1 = tf.keras.layers.Dropout(0.1)\n",
    "    self.dropout2 = tf.keras.layers.Dropout(0.1)\n",
    "    self.layer_normalization1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layer_normalization2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "  def call(self, data, training):\n",
    "    x = self.head_attention_layer(data, data, use_causal_mask=True)\n",
    "    x = self.dropout1(x, training=training)\n",
    "    x = tf.math.add(x, data)\n",
    "    x = self.layer_normalization1(x)\n",
    "    y = self.dense(x)\n",
    "    y = self.dropout2(y, training=training)\n",
    "    x = tf.math.add(x, y)\n",
    "    return self.layer_normalization2(x)"
   ],
   "metadata": {
    "id": "IWV6ajI5cFap"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Bible model"
   ],
   "metadata": {
    "collapsed": false,
    "id": "U1p78otwcFap"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [],
   "source": [
    "class BibleModel(tf.keras.Model):\n",
    "  def __init__(self, tokenizer, optimizer=tf.keras.optimizers.Adam(), loss_function=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)):\n",
    "    super(BibleModel, self).__init__()\n",
    "    self.tokenizer = tokenizer\n",
    "    self.optimizer = optimizer\n",
    "    self.loss_function = loss_function\n",
    "    self.metrics_list = [\n",
    "      tf.keras.metrics.Mean(name=\"loss\"),\n",
    "      tf.keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
    "      tf.keras.metrics.SparseTopKCategoricalAccuracy(3, name=\"top-3-acc\")\n",
    "    ]\n",
    "\n",
    "    self.layer = tf.keras.Sequential([\n",
    "      BibleEmbedding(),\n",
    "      TransformerBlock(),\n",
    "      tf.keras.layers.Dense(VOCABULARY_SIZE, activation=None)\n",
    "    ])\n",
    "\n",
    "  def call(self, data, training=True):\n",
    "    return self.layer(data)\n",
    "\n",
    "  def reset_metrics(self):\n",
    "    for metric in self.metrics:\n",
    "        metric.reset_states()\n",
    "\n",
    "  @tf.function\n",
    "  def train_step(self, data):\n",
    "    x, targets = data\n",
    "\n",
    "    # compute output and loss, train the variables\n",
    "    with tf.GradientTape() as tape:\n",
    "      predictions = self(x, training=True)\n",
    "      loss = self.loss_function(targets, predictions) + tf.reduce_sum(self.losses)\n",
    "\n",
    "    # update trainable variables\n",
    "    gradients = tape.gradient(loss, self.trainable_variables)\n",
    "    self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "\n",
    "    # update metrics\n",
    "    self.metrics_list[0].update_state(loss)\n",
    "\n",
    "    for metric in self.metrics_list[1:]:\n",
    "        metric.update_state(targets, predictions)\n",
    "      \n",
    "    # return a dict with metric information\n",
    "    return {m.name : m.result() for m in self.metrics_list}\n",
    "\n",
    "  def generate_text(self, prompt, output_length, top_k):\n",
    "    generated = prompt\n",
    "    for i in range(output_length):\n",
    "        tokenized_prompt = self.tokenizer.tokenize(generated)\n",
    "        output = self(tf.expand_dims(tokenized_prompt, 0), training=False)\n",
    "        logits = output[:, -1, :]  # select the last token's logits\n",
    "        filtered_logits, top_indices = tf.math.top_k(logits, k=top_k, sorted=True)\n",
    "        chosen_index = tf.random.categorical(filtered_logits, num_samples=1)[-1, 0].numpy()\n",
    "        generated += ' ' + self.tokenizer.detokenize(tf.expand_dims(top_indices[0][chosen_index], 0)).numpy().decode(\"utf-8\")\n",
    "    return generated"
   ],
   "metadata": {
    "id": "fCpw7kfXcFap"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training loop"
   ],
   "metadata": {
    "id": "kOAv_LQM1VKc"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def training_loop(model, train_ds, train_summary_writer):\n",
    "  for epoch in range(EPOCHS):\n",
    "    print(f\"Epoch {epoch}:\")\n",
    "    \n",
    "    for data in tqdm.tqdm(train_ds, position=0, leave=True):\n",
    "        metrics = model.train_step(data)\n",
    "        \n",
    "    with train_summary_writer.as_default():\n",
    "        for metric in model.metrics:\n",
    "            tf.summary.scalar(f\"{metric.name}\", metric.result(), step=epoch)\n",
    "\n",
    "    # print metrics\n",
    "    print(', '.join(map(lambda metric: f\"{metric.name}: {'{:.2f}'.format(metric.result())}\", model.metrics)))\n",
    "\n",
    "    # reset all metrics (requires a reset_metrics method in the model)\n",
    "    model.reset_metrics()    \n",
    "    \n",
    "    # Validation: text generation\n",
    "    prediction = model.generate_text(\"What is\", 15, 3)\n",
    "    print(\"Prediction: \", prediction)\n",
    "\n",
    "    model.reset_metrics()\n",
    "    print(\"\\n\")"
   ],
   "metadata": {
    "id": "p7Fp7VkG1WqB"
   },
   "execution_count": 111,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model = BibleModel(sp_tokenizer)\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "train_log_dir = 'logs/' + current_time + '/train'\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "\n",
    "training_loop(model, train_dataset, train_summary_writer)"
   ],
   "metadata": {
    "id": "oY8ztWzO5tvj",
    "outputId": "d3021a05-62fd-45f4-a2d4-999185bc544c",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 834
    }
   },
   "execution_count": 112,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 882/882 [03:03<00:00,  4.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 4.96, accuracy: 0.17, top-3-acc: 0.29\n",
      "Prediction:  What is a d il of his name and the earth and the sea l a great\n",
      "\n",
      "\n",
      "Epoch 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 882/882 [02:57<00:00,  4.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 4.37, accuracy: 0.21, top-3-acc: 0.35\n",
      "Prediction:  What is a d am the third day and the third day and he was a great\n",
      "\n",
      "\n",
      "Epoch 2:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 882/882 [03:03<00:00,  4.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 4.18, accuracy: 0.23, top-3-acc: 0.37\n",
      "Prediction:  What is in the earth and the earth and the angel said unto him and the earth\n",
      "\n",
      "\n",
      "Epoch 3:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 882/882 [02:58<00:00,  4.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 4.08, accuracy: 0.24, top-3-acc: 0.38\n",
      "Prediction:  What is and i saw a loud voice and a voice and a voice of the voice\n",
      "\n",
      "\n",
      "Epoch 4:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 882/882 [02:59<00:00,  4.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 4.02, accuracy: 0.24, top-3-acc: 0.39\n",
      "Prediction:  What is in the earth and i saw the angel sound of the trumpet and they said\n",
      "\n",
      "\n",
      "Epoch 5:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 882/882 [03:04<00:00,  4.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 3.98, accuracy: 0.25, top-3-acc: 0.40\n",
      "Prediction:  What is the throne of the earth and the angel and the lamb and the lamb shall\n",
      "\n",
      "\n",
      "Epoch 6:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 882/882 [03:05<00:00,  4.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 3.94, accuracy: 0.25, top-3-acc: 0.40\n",
      "Prediction:  What is in heaven and earth shall come and the earth and shall be a great sea\n",
      "\n",
      "\n",
      "Epoch 7:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 882/882 [03:04<00:00,  4.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 3.92, accuracy: 0.25, top-3-acc: 0.41\n",
      "Prediction:  What is in the earth and the angel and he said unto him that sitteth in heaven\n",
      "\n",
      "\n",
      "Epoch 8:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 882/882 [03:05<00:00,  4.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 3.89, accuracy: 0.26, top-3-acc: 0.41\n",
      "Prediction:  What is in the midst of the earth and the earth and the earth and the flood\n",
      "\n",
      "\n",
      "Epoch 9:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 882/882 [03:01<00:00,  4.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 3.88, accuracy: 0.26, top-3-acc: 0.41\n",
      "Prediction:  What is the voice of the angel and the voice of the angel of the heaven and\n",
      "\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [
    {
     "data": {
      "text/plain": "Launching TensorBoard..."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n      <iframe id=\"tensorboard-frame-f83024b3a28c88b7\" width=\"100%\" height=\"800\" frameborder=\"0\">\n      </iframe>\n      <script>\n        (function() {\n          const frame = document.getElementById(\"tensorboard-frame-f83024b3a28c88b7\");\n          const url = new URL(\"/\", window.location);\n          const port = 6006;\n          if (port) {\n            url.port = port;\n          }\n          frame.src = url;\n        })();\n      </script>\n    "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ki_klausur",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ccc2749a7be4b92b5b584e86e3aa803380c51e1da575f498b6f2d8cfba82568a"
   }
  },
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
